---
title: "solution_script"
author: "Charles"
date: "19 8 2022"
output: html_document
editor_options: 
  chunk_output_type: console
---

## **Description**

The task consists of finding groups or clusters within a dataset consisting of 49 patients (observations) who provided various responses about 12 factors (variables) that motivate them. Each value is basically the mean value of about 7 questions people answered in the particular category/variable. 

This is basically an unsupervised clustering problem. Clustering is a form of exploratory data analysis (EDA) whereby observations are grouped into meaningful groups, based on features they share.

I will use various approaches to identify and confirm the optimal number of groups of patients within the dataset. These approaches will fall under the following milestones:

1. Data pre-processing
2. Select similarity metric
3. Clustering
4. Analyses

## **Setup**

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = TRUE)
```

## **Load libraries**

```{r libraries}
library(openxlsx)
library(ggplot2)
library(purrr)
library(dplyr)
library(tidyr)
```

## **Load in the dataset**

```{r data_set}
list.files("raw_data")

# read in the data
raw_data <- read.xlsx("raw_data/Vinyu_Motivation_Challenge_Test.xlsx", 
                      rowNames = TRUE)
```

## **Step 1: Data pre-processing.**

This is to explore for missing values and to make sure the data are on the same scale. This is because based on my presumption, these values were very unlikely to be on the same scale.

### ***i. Check for missing values.**

```{r check_NA}
anyNA(raw_data) # check for NAs

# inspect
View(raw_data) 
dim(raw_data)
```

Result shows the absence of missing values in the dataset.

### **ii. Standardise the the features.**

I will perform standardisation. This will help make the data comparable, since it is unlikely that they were measured on the same scale. Without standardisation, distances between samples may become exagerated and unmeaningful. I will make use of base R's $scale()$ function. Each feature column will be normalised to a mean of 0 and a variance of 1.

```{r scaling}
# scale the data
scaled_data <- scale(raw_data)
```

## **Step 2: The similarity metric.**

The similarity metric I will use is the distance. So I will compute the distance between each samples, using base R's $dist()$ function which calculates the Euclidean distance by default.

```{r dist}
# calculate the distance
dist_scaled_data <- dist(scaled_data)
```

## **Step 3: Clustering.**

### **i. Hierarchical clustering.**

We can perform hierarchical clustering based on the distances defined above using the $hclust()$ function. This function returns an `hclust` object that describes the groupings that were created using the algorithm described above. The `plot` method represents these relationships with a tree or dendrogram: 


```{r Hclust}
library(rafalib) # to optimise graphical parameters.
mypar()
hc <- hclust(dist_scaled_data)
hc
plot(hc, cex = 0.5, asp = 1)
```

The hierarchical clustering is consistent with the existence to two main clusters, one of which has two main sub-clusters. So I define three clusters, and I cut the tree at height value 8 group all samples that are within that distance into groups below. To visualize this, I will draw a horizontal line at the height I wish to cut and this defines that line. I start with 8.

```{r clust_assignment}
myplclust(hc,cex = 0.5)
abline(h = 8)

# cut tree
hclusters <- cutree(hc, h = 9)
print(hclusters)
```

Results show which cluster each patient belongs to.

### **ii. K-Means clustering.**

We can also cluster with the `kmeans` function to perform k-means clustering. I will estimate the otimal number of Ks using two appraoches: 

#### ** Estimation by total within-cluster sum of squares**

I will use $map_dbl()$ from the $purrr$ library to run k-means using values of k ranging from 1 to 10 and extract the total within-cluster sum of squares.

```{r within_cluster}
# Use map_dbl to run many models with varying value of k (centers)
tot_withinss <- map_dbl(1:10,  function(k){
  model <- kmeans(x = scaled_data, centers = k)
  model$tot.withinss
})

# Generate a data frame containing both k and tot_withinss
elbow_df <- data.frame(
  k = 1:10,
  tot_withinss = tot_withinss
)

# Plot the elbow plot
ggplot(elbow_df, aes(x = k, y = tot_withinss)) +
  ggtitle("within-cluster sum of squares vs K") +
  geom_point() +
  geom_line() +
  scale_x_continuous(breaks = 1:10) +
  theme_bw() +
  theme(aspect.ratio = 1)
```

The plot of the total within-cluster sum of squares **tot_withinss**, against the number of clusters **k**. The curve begins to flatten out at k = 3. So I use 3 as the estimated value of k.

#### **Estimation by Silhouette analysis.**

```{r silhouette}
library(cluster)

# Use map_dbl to run many models with varying value of k
sil_width <- map_dbl(2:10,  function(k){
  model <- pam(x = scaled_data, k = k)
  model$silinfo$avg.width
})

# Generate a data frame containing both k and sil_width
sil_df <- data.frame(
  k = 2:10,
  sil_width = sil_width
)

# Plot the relationship between k and sil_width
ggplot(sil_df, aes(x = k, y = sil_width)) +
  geom_line() +
  scale_x_continuous(breaks = 2:10)
```

The highest silhouette width is 0.12 corresponsing to a k of 2. This is in line with results from the hclust and tot_withinss. So I use a K = 3.

```{r kmeans}
# use an mds plot to visualise the kmeans clustering
km <- kmeans(scaled_data, centers = 3)
mds <- cmdscale(dist_scaled_data)

# create data frame
mds <- as.data.frame(mds)

colnames(mds) <- c("MDS1", "MDS2")

mds$cluster <- km$cluster

ggplot(mds, aes(MDS1, MDS2)) +
  geom_point(aes(color = factor(cluster))) +
  ggtitle("K - Means clustering") +
  theme_bw() +
  theme(aspect.ratio = 1)
```


## **Step 4: Analyses.**

### **Perform principal components analysis to identify influential features.**

```{r PCA}
# do PCA: I fit in the raw data 
# because I set scale parameter to True
pca <- prcomp(raw_data, scale. = TRUE)

```