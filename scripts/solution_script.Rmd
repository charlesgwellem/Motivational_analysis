---
title: "solution_script"
author: "Charles"
date: "19 8 2022"
output: html_document
editor_options: 
  chunk_output_type: console
---

## **Description**

The task consists of finding groups or clusters within a dataset consisting of 49 patients (observations) who provided various responses about 12 factors (variables) that motivate them. Each value is basically the mean value of about 7 questions people answered in the particular category/variable. 

This is basically an unsupervised clustering problem. Clustering is a form of exploratory data analysis (EDA) whereby observations are grouped into meaningful groups, based on features they share.

I will use various approaches to identify and confirm the optimal number of groups of patients within the dataset. These approaches will fall under the following milestones:

1. Data pre-processing
2. Select similarity metric
3. Clustering
4. Analyses

## **Setup**

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = TRUE)
```

## **Load libraries**

```{r libraries}
library(openxlsx)
library(ggplot2)
library(purrr)
library(dplyr)
library(tidyr)
```

## **Load in the dataset**

```{r data_set}
list.files("raw_data")

# read in the data
raw_data <- read.xlsx("raw_data/Vinyu_Motivation_Challenge_Test.xlsx", 
                      rowNames = TRUE)
```

## **Step 1: Data pre-processing.**

This is to explore for missing values and to make sure the data are on the same scale. This is because based on my presumption, these values were very unlikely to be on the same scale.

### ***i. Check for missing values.**

```{r check_NA}
anyNA(raw_data) # check for NAs

# inspect
View(raw_data) 
dim(raw_data)
```

Result shows the absence of missing values in the dataset.

### **ii. Standardise the the features.**

I will perform standardisation. This will help make the data comparable, since it is unlikely that they were measured on the same scale. Without standardisation, distances between samples may become exagerated and unmeaningful. I will make use of base R's $scale()$ function. Each feature column will be normalised to a mean of 0 and a variance of 1.

```{r scaling}
# scale the data
scaled_data <- scale(raw_data)
```

## **Step 2: The similarity metric.**

The similarity metric I will use is the distance. So I will compute the distance between each samples, using base R's $dist()$ function which calculates the Euclidean distance by default.

```{r dist}
# calculate the distance
dist_scaled_data <- dist(scaled_data)
```

## **Step 3: Clustering.**

### **i. Hierarchical clustering.**

We can perform hierarchical clustering based on the distances defined above using the $hclust()$ function. This function returns an `hclust` object that describes the groupings that were created using the algorithm described above. The `plot` method represents these relationships with a tree or dendrogram: 


```{r Hclust}
library(rafalib) # to optimise graphical parameters.
mypar()
hc <- hclust(dist_scaled_data)
hc
plot(hc, cex = 0.5, asp = 1)
```

The hierarchical clustering is consistent with the existence to two main clusters, one of which has two main sub-clusters. So I define three clusters, and I cut the tree at height value 8 group all samples that are within that distance into groups below. To visualize this, I will draw a horizontal line at the height I wish to cut and this defines that line. I start with 8.

```{r clust_assignment}
myplclust(hc,cex = 0.5)
abline(h = 8)

# cut tree
hclusters <- cutree(hc, h = 9)
print(hclusters)
```

Results show which cluster each patient belongs to.

### **ii. K-Means clustering.**

We can also cluster with the `kmeans` function to perform k-means clustering. I will estimate the otimal number of Ks using two appraoches: 

#### ** Estimation by total within-cluster sum of squares**

I will use $map_dbl()$ from the $purrr$ library to run k-means using values of k ranging from 1 to 10 and extract the total within-cluster sum of squares.

```{r within_cluster}
# Use map_dbl to run many models with varying value of k (centers)
tot_withinss <- map_dbl(1:10,  function(k){
  model <- kmeans(x = scaled_data, centers = k)
  model$tot.withinss
})

# Generate a data frame containing both k and tot_withinss
elbow_df <- data.frame(
  k = 1:10,
  tot_withinss = tot_withinss
)

# Plot the elbow plot
ggplot(elbow_df, aes(x = k, y = tot_withinss)) +
  ggtitle("within-cluster sum of squares vs K") +
  geom_point() +
  geom_line() +
  scale_x_continuous(breaks = 1:10) +
  theme_bw() +
  theme(aspect.ratio = 1)
```

The plot of the total within-cluster sum of squares **tot_withinss**, against the number of clusters **k**. The curve begins to flatten out at k = 3. So I use 3 as the estimated value of k.

#### **Estimation by Silhouette analysis.**

```{r silhouette}
library(cluster)

# Use map_dbl to run many models with varying value of k
sil_width <- map_dbl(2:10,  function(k){
  model <- pam(x = scaled_data, k = k)
  model$silinfo$avg.width
})

# Generate a data frame containing both k and sil_width
sil_df <- data.frame(
  k = 2:10,
  sil_width = sil_width
)

# Plot the relationship between k and sil_width
ggplot(sil_df, aes(x = k, y = sil_width)) +
  geom_line() +
  scale_x_continuous(breaks = 2:10)
```

The highest silhouette width is 0.12 corresponsing to a k of 2. This is in line with results from the hclust and tot_withinss. So I use a K = 3.

```{r kmeans}
# use an mds plot to visualise the kmeans clustering
km <- kmeans(scaled_data, centers = 3)
mds <- cmdscale(dist_scaled_data)

# create data frame
mds <- as.data.frame(mds)

colnames(mds) <- c("MDS1", "MDS2")

mds$cluster <- km$cluster

ggplot(mds, aes(MDS1, MDS2)) +
  geom_point(aes(color = factor(cluster))) +
  ggtitle("K - Means clustering visualisation") +
  theme_bw() +
  theme(aspect.ratio = 1,
        plot.title=element_text(hjust=0.5)) # centralise plot title
```


## **Step 4: Analyses.**

### **Perform principal components analysis to identify influential features.**

I perform PCA to identify features that determine the positions of the patients on the principal components and also if there are hidden structures within the data set.

```{r PCA}
# do PCA: I fit in the raw data 
# because I set scale parameter to True
pca <- prcomp(raw_data, scale. = TRUE)

# obtain variance of each PC
pcaVar <- pca$sdev ** 2

# obtain pct variance from each PC
pctVar <- round(pcaVar / sum(pcaVar) * 100, 1) 

pctVar <- as.data.frame(pctVar)

pctVar$PC <- 1:dim(pctVar)[1]

# make scree plot
ggplot(pctVar, aes(PC, pctVar)) +
  ggtitle("scree plot") +
  geom_bar(stat = "identity") +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  theme_bw() +
  theme(aspect.ratio = 1,
        plot.title=element_text(hjust=0.5)) # centralise plot title
```

The scree plot shows that PC1 accounts for almost 30% of the total variance. But PC2 and PC2 also account for a considerable amount of variance being 16% and 13% respectively. This necessitates further analyses to discover more structures within the dataset. This will be done using the surrogate value analysis or sva.

Plotting the PCA.

```{r pca_plot}
# prepare dataframe of PCA details
pca_df <- data.frame(samples = rownames(pca$x),
                     X = pca$x[, 1],
                     Y = pca$x[, 2])

# add cluster information
pca_df$cluster <- km$cluster

ggplot(pca_df, aes(x = X, y = Y,
                   label = samples),
       fill = cluster) +
  geom_text(aes(colour = factor(cluster))) +
  xlab(paste0("PC1 - ", pctVar[1, 1], "%")) +
  ylab(paste0("PC2 - ", pctVar[2, 1], "%")) +
  theme_bw() +
  theme(aspect.ratio = 1)
```

Just like the hierarchical clustering, the PCA plot of the first two principal comonents show that P1, P23, and P44 form a cluster separated from the rest. However at the PCA level, P34 appears to highly correlate with P1, P23 and P44, a feature which hierarchical clustering misses out.

To find out the most influential features, the loading scores need to be assessed.

```{r loading_scores}
loading_scores <- pca$rotation[, 1]
feature_scores <- abs(loading_scores)
feature_scores_ranked <- sort(feature_scores,
                              decreasing = TRUE)
loading_scores[names(feature_scores_ranked)]
```

I will only assess the loading scores of the first PC since it accounts for most of the variation in the data set. The loading scores show that thet variables **Macht_und_Einfluss**,  **Eros_und_Schoenheit**, **Wettkampf** and **Essen** tend to have a strong influence in pushing samples to the right of the PC1 vs PC1 plot. On the other hand **Emotionale_Ruhe** tend to push samples to the left of the PC1 vs PC2 plot. This shows that the factors  **Macht_und_Einfluss**,  **Eros_und_Schoenheit**, **Wettkampf**, **Essen** and **Emotionale_Ruhe** are very strong motivation factors.

### **Perform the surrogate value analysis.**

To identify more hidden strictures within the dataet, I perform sva.
